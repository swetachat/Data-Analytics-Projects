{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Naives Bayes, Decision Tree, SVM, and LR Models using Ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!pip install BeautifulSoup4\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import requests\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "import re\n",
    "import random\n",
    "import collections\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import bigrams, trigrams               # NLTK has modules that can generate a list of bigrams and trigrams\n",
    "\n",
    "# For model build\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.classify import DecisionTreeClassifier\n",
    "from nltk.classify import MaxentClassifier       # For Logistics Regression model\n",
    "# SVM model\n",
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.svm import SVC\n",
    "from nltk.metrics.scores import (accuracy, precision, recall, f_measure)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the bag_of_words function that will take a list and return\n",
    "# a dictionary. The dictionary will contain each of the words in the list\n",
    "# and it will have the value True assigned to each word\n",
    "def bag_of_words(ngram_list):\n",
    "    return dict([(ngram, True) for ngram in ngram_list])\n",
    "\n",
    "# Functions to return a list of bigrams\n",
    "def bigramReturner (list_of_words):\n",
    "    bigramFeatureVector = []\n",
    "    for item in bigrams(list_of_words):\n",
    "        bigramFeatureVector.append(' '.join(item))\n",
    "    return bag_of_words(bigramFeatureVector)\n",
    "  \n",
    "# Functions to return a list of trigrams\n",
    "def trigramReturner (list_of_words):\n",
    "    trigramFeatureVector = []\n",
    "    for item in trigrams(list_of_words):\n",
    "        trigramFeatureVector.append(' '.join(item))\n",
    "    return bag_of_words(trigramFeatureVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#a. Remove punctuation.\n",
    "#b. Replace contractions.\n",
    "#c. Convert all text to lower case.\n",
    "#d. Remove stop words.\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Removing negative words from english_stops, as this is  important keywords for negative sentiments\n",
    "english_stops = set(stopwords.words('english'))\n",
    "negate_words = [\"doesn't\",\"no\",\"not\",\"didn't\",\"don't\",\"haven't\",\"shouldn't\",\"weren't\",\"won't\"]\n",
    "english_stops = [word for word in english_stops if word not in negate_words]\n",
    "\n",
    "def decontraction(review):    \n",
    "    review = re.sub(r\"won't\", \"will not\", review)\n",
    "    review = re.sub(r\"can\\'t\", \"can not\", review)\n",
    "    review = re.sub(r\"ain't\", \"am not\", review)\n",
    "    review = re.sub(r\"let's\", \"let us\", review)    \n",
    "\n",
    "    review = re.sub(r\"n\\'t\", \" not\", review)\n",
    "    review = re.sub(r\"\\'re\", \" are\", review)\n",
    "    review = re.sub(r\"\\'s\", \" is\", review)\n",
    "    review = re.sub(r\"\\'d\", \" would\", review)\n",
    "    review = re.sub(r\"\\'ll\", \" will\", review)\n",
    "    review = re.sub(r\"\\'t\", \" not\", review)\n",
    "    review = re.sub(r\"\\'ve\", \" have\", review)\n",
    "    review = re.sub(r\"\\'m\", \" am\", review)\n",
    "    review = re.sub(r\"\\'cause\", \"because\", review)\n",
    "    review = re.sub(r\"\\..\", \"\", review)                   #replacing .. or ...\n",
    "    \n",
    "    return review\n",
    "\n",
    "\n",
    "def cleanData(review):\n",
    "    review = decontraction(review)\n",
    "    words = word_tokenize(review)\n",
    "    words = [word.lower() for word in words]\n",
    "    words = [word for word in words if word not in english_stops]\n",
    "    words = [word for word in words if word not in set(string.punctuation[1:])]\n",
    "\n",
    "    stem_list = [stemmer.stem(word) for word in words]\n",
    "    return stem_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scrape the reviews and the associated ratings\n",
    "# Recode ratings as either positive (4-5) or negative (1-3\n",
    "\n",
    "def scrapeReviewRating(hotelName):\n",
    "    reviewList = []    \n",
    "    prefix = \"https://www.yelp.com/biz/\"\n",
    "    postfix = \"?start=\"\n",
    "    all_dataset = []\n",
    "    \n",
    "    # Headers will make it look like you are using a web browser\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.90 Safari/537.34'}\n",
    "      \n",
    "    for i in range(0,500,20):        \n",
    "        url = prefix + hotelName + postfix +str(i)\n",
    "        response = requests.get(url, headers=headers, verify=False).text\n",
    "        \n",
    "        # Create a soup object and find all 500 reviews\n",
    "        soup = BeautifulSoup(response, \"lxml\")\n",
    "        \n",
    "        # Find the review blocks\n",
    "        review_blocks = soup.find_all('div', 'review-content')\n",
    "        \n",
    "        for r in review_blocks:\n",
    "            #scrape reviews and ratings\n",
    "            review = r.p.text                        \n",
    "            int_rating = float(r.find('div',{'class':'i-stars'}).get('title').split(' ')[0])\n",
    "                     \n",
    "            # Recode 1-3 rating as neg and 4-5 as pos\n",
    "            rating = \"neg\" if int_rating < 4 else \"pos\"\n",
    "            \n",
    "            all_dataset.append((rating,review))       #create list of tuples with (string,string)\n",
    "    print(\"Dataset Created\")      \n",
    "    \n",
    "    return all_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build  naÃ¯ve Bayes, decision tree, logistic regression and SVM models\n",
    "\n",
    "def getMatrix(classifier, test_set, datatype, classifierName):\n",
    "    refsets = collections.defaultdict(set)\n",
    "    testsets = collections.defaultdict(set)\n",
    "   \n",
    "    #create ref and test datasets\n",
    "    for i, (feats, label) in enumerate(test_set):\n",
    "        refsets[label].add(i)\n",
    "        observed = classifier.classify(feats)\n",
    "        testsets[observed].add(i)\n",
    "    \n",
    "    df ={\n",
    "            \"Classifier\":classifierName,\n",
    "            \"Data_type\":datatype, \n",
    "            \"Accuracy\": nltk.classify.util.accuracy(classifier, test_set),\n",
    "            \"pos precision\": precision(refsets['pos'], testsets['pos']),\n",
    "            \"pos recall\": recall(refsets['pos'], testsets['pos']),\n",
    "            \"pos F-measure\": f_measure(refsets['pos'], testsets['pos']),\n",
    "            \"neg precision\": precision(refsets['neg'], testsets['neg']),\n",
    "            \"neg recall\": recall(refsets['neg'], testsets['neg']),\n",
    "            \"neg F-measure\": f_measure(refsets['neg'], testsets['neg'])\n",
    "    }\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build model and Performance metrics\n",
    "def buildModel(train_set, test_set, datatype):\n",
    "    \n",
    "    # Train the Naive Bayes model\n",
    "    nb_classifier = NaiveBayesClassifier.train(train_set)\n",
    "    metrics.append(getMatrix(nb_classifier,test_set,datatype,\"Naive Bayes\"))\n",
    "   \n",
    "    # Train a decision tree model\n",
    "    dt_classifier = DecisionTreeClassifier.train(train_set, binary=True, entropy_cutoff=0.8, depth_cutoff=5, support_cutoff=30)\n",
    "    metrics.append(getMatrix(dt_classifier,test_set,datatype,\"Decision Tree\"))\n",
    "   \n",
    "    # Train Logistic regression model\n",
    "    logit_classifier = MaxentClassifier.train(train_set, algorithm='gis', trace=0, max_iter=10, min_lldelta=0.5)\n",
    "    metrics.append(getMatrix(logit_classifier,test_set,datatype,\"Logistics Regression\"))\n",
    "    \n",
    "    # Train the SVM model\n",
    "    SVM_classifier = SklearnClassifier(SVC(), sparse=False).train(train_set)\n",
    "    metrics.append(getMatrix(SVM_classifier,test_set,datatype,\"Support Vector Machine\"))\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\susha\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "C:\\Users\\susha\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "C:\\Users\\susha\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "C:\\Users\\susha\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "C:\\Users\\susha\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "C:\\Users\\susha\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "C:\\Users\\susha\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "C:\\Users\\susha\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "C:\\Users\\susha\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "C:\\Users\\susha\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "C:\\Users\\susha\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "C:\\Users\\susha\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "C:\\Users\\susha\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "C:\\Users\\susha\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "C:\\Users\\susha\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "C:\\Users\\susha\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "C:\\Users\\susha\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "C:\\Users\\susha\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "C:\\Users\\susha\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "C:\\Users\\susha\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "C:\\Users\\susha\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "C:\\Users\\susha\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "C:\\Users\\susha\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "C:\\Users\\susha\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "C:\\Users\\susha\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "C:\\Users\\susha\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Created\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "unigram_dataset = []\n",
    "bigram_dataset = []\n",
    "triigram_dataset = []\n",
    "ngram_dataset = []\n",
    "\n",
    "dataset = scrapeReviewRating(\"white-manna-hackensack\")\n",
    "\n",
    "# Create Unigram dataset\n",
    "unigram_dataset = [(bag_of_words(cleanData(dataset[i][1])), dataset[i][0]) for i in range(len(dataset))] \n",
    "\n",
    "# Create bigram dataset\n",
    "bigram_dataset = [(bigramReturner(cleanData(dataset[i][1])), dataset[i][0]) for i in range(len(dataset))] \n",
    "\n",
    "# Create trigram dataset\n",
    "trigram_dataset = [(trigramReturner(cleanData(dataset[i][1])), dataset[i][0]) for i in range(len(dataset))] \n",
    "\n",
    "# Create ngram dataset\n",
    "ngram_dataset =unigram_dataset + bigram_dataset + trigram_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-535d29607a6d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Shuffle Dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munigram_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbigram_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrigram_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'random' is not defined"
     ]
    }
   ],
   "source": [
    "metrics = []\n",
    "\n",
    "# Shuffle Dataset\n",
    "random.shuffle(unigram_dataset)\n",
    "random.shuffle(bigram_dataset)\n",
    "random.shuffle(trigram_dataset)\n",
    "random.shuffle(ngram_dataset)\n",
    "\n",
    "train_unigramDataset =[]\n",
    "\n",
    "# Split dataset into 70/30 and create the training and test dataset\n",
    "train_unigramDataset, test_unigramDataset = unigram_dataset[:int(len(unigram_dataset)*.70)], unigram_dataset[int(len(unigram_dataset)*.70):]\n",
    "train_bigramDataset, test_bigramDataset = bigram_dataset[:int(len(bigram_dataset)*.70)], bigram_dataset[int(len(bigram_dataset)*.70):]\n",
    "train_trigramDataset, test_trigramDataset = trigram_dataset[:int(len(trigram_dataset)*.70)], trigram_dataset[int(len(trigram_dataset)*.70):]\n",
    "train_ngramDataset, test_ngramDataset = ngram_dataset[:int(len(ngram_dataset)*.70)], ngram_dataset[int(len(ngram_dataset)*.70):]\n",
    "\n",
    "# Build Model/Metrics using unigrams\n",
    "buildModel(train_unigramDataset, test_unigramDataset,\"Unigrams\")\n",
    "\n",
    "# Build Model/Metrics using bigrams\n",
    "buildModel(train_bigramDataset, test_bigramDataset,\"Bigrams\")\n",
    "\n",
    "# Build Model/Metrics using trigrams\n",
    "buildModel(train_trigramDataset, test_trigramDataset,\"Trigrams\")\n",
    "\n",
    "# Build Model/Metrics using nigrams\n",
    "buildModel(train_ngramDataset, test_ngramDataset,\"Ngrams\")\n",
    "\n",
    "metrics_df =pd.DataFrame(metrics)\n",
    "metrics_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarizing Model Performance Metrics.\n",
    "\n",
    "Naive Bayes model produces the best negative review recall for bigrams, trigrams and ngrams.\n",
    "\n",
    "Also, Naive Bayes gives 100% neg review recall for unigrams, which shouldn't be possible for a model. One of the reason could be imbalance dataset. Also, using unigram, Naive Bayes should have produce high TP and very low FN, which makes model 100% recall.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
